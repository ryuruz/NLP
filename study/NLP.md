# 한국어 임베딩
### 임베딩(embedding)

+ 자연어를 숫자의 나열인 벡터로 바꾼 결과 혹은 그 일련의 과정 전체를 가리키는 용어
+ 말뭉치(corpus)의 의미, 문법 정보가 응축되어 있음
+ 벡터 -> 사칙연산 가능, 단어/문서 관련도(relevance) 계산 가능
+ 임베딩을 통해 __전이학습__ 가능
  + 전이 학습(Transfer Learning) : 특정 문제를 풀기 위해 학습한 모델을 다른 문제를 푸는 데에 재사용하는 기법
  + 대규모 말뭉치를 pretrain한 임베딩을 문서 분류 모델의 입력값으로 쓰고, 해당 임베딩을 포함한 모델 전체를 문서 분류 과제를 잘 할 수 있도록 fine tunning하는 방식

# XLNet
+ 2019년 구글 연구팀이 발표한 기법

+ 이 연구팀에서 임베딩의 최근 흐름을 크게 두 가지로 구분
  + AR(AutoRegressive) model : 데이터를 순차적으로 처리하는 기법의 총칭
    + ELMo, GPT 등 : 이전 문맥을 바탕으로 다음 단어를 예측하는 과정에서 학습하는 모델
  + 문맥을 양방향(bidirectional)으로 볼 수 없는 한계가 있음
  + AE(AutoEncoding) model : 입력값을 복원하는 기법을 두루 일컬음
    + BERT : 문장 일부에 노이즈(마스킹)를 주어 문장을 원래대로 복원하는 과정에서 학습하는 모델
      + 마스킹 처리된 단어가 어떤 단어일지 맞추는 것에 포커스를 둔다는 점에서 DAE라고 표현하기도 함
    + 양방향 모델이기는 하나, 마스킹 처리한 토큰들을 서로 독립으로 가정하기 때문에 각 토큰들의 의존 관계를 따질 수 없다는 한계가 있음
    
+ 두 모델의 한계를 극복하기 위해 Permutation Language model을 제안
  + 토큰을 랜덤으로 셔플한 뒤 그 뒤바뀐 순서를 기반으로 언어 모델을 학습하는 기법
  + 특정 토큰을 예측할 때 문장 전체 문맥을 살필 수 있게 됨
    + 해당 토큰을 제외한 문장의 부분집합 전부를 학습할 수 있다는 것
  + 단어 간 의존관계 포착에 유리
  
  ex) [1,2,3,4]의 토큰 4 개짜리 문장을 랜덤으로 뒤섞은 결과가 [3,2,4,1]일 때, 셔플된 시퀀스의 첫 번째 단어인 3번 토큰을 맞춰야 하는 상황일 때는 2,4,1 토큰은 3번 토큰이 등장한 후 나온 단어들이므로 입력으로 줄 수 없고 정답인 3번 토큰의 정보도 줄 수 없으므로 이런 상황에서는 이전 segment의 메모리 정보를 사용하게 된다.
  
  ex) 같은 문장을 셔플하여 [2,4,3,1]의 결과가 나왔고, 이 상황에서도 역시 3번 토큰을 맞춰야하는 상황이라면 메모리, 2번, 4번 토큰이 입력된다.
  
  + 퍼뮤테이션 언어 모델의 실제 구현은 토큰을 뒤섞는 것이 아니라 attention mask를 이용하여 실현됨
  + 랜덤 셔플 결과가 [3,2,4,1]과 [3,2,1,4]이고 세번째 토큰을 예측해야 할 경우, [3,2]의 같은 입력을 이용하여 다른 출력을 내야하는 모순이 존재
    + two-stream self attention 기법을 이용하여 해결
      + query stream과 content stream 두 가지를 혼합한 self attention 기법
        + query stream : 토큰과 위치 정보를 활용한 selg attention 기법
        + content stream : 기존 트랜스포머 네트워크와 거의 유사한 기법

+ Transformer-XL : XLNet 이전에 발표된 모델로, Yang et al. 연구팀에서는 이 모델의 segment recurrence와 relative position embedding 기법을 차용함
  + Segment recurrence : 기존 트랜스포머 네트워크에서 고정된 길이의 문맥 정보만 활용할 수 있다는 단점을 보완하여, 좀 더 긴 context를 활용하기 위해 제안된 기법 
    + 1. 우선 문서를 작은 segment 단위로 자른 후, 첫 번째 segment를 기존 트랜스포머 네트워크처럼 충분히 학습시킨 후 저장(cache)
    + 2. 두 번째 segment를 학습시키며, 이번 경우에는 첫 번째 segment 정보를 활용하여 계산함
      + 현재 segment를 학습할 때 고려하는 직전 segment 계산 결과를 memory라고 부름
  + 단어 쌍 사이의 거리 정보인 상대 위치를 활용함  
    + 상대 위치는 음수값이 존재하지 않음
    
### 모델 구현 
+ 생략


# Bi-LSTM Hegemony

+ Chistopher Manning 교수 2017/10/20 강연 내용
+ task가 무엇이든지 간에 attention이 적용된 BiLSTM 모델이 자연어처리 분야에서 최고 성능을 낸다
___
+ Vanilla RNN은 grdient vanishing/exploding 문제에 취약한 구조를 가지고 있음
+ LSTM (Long Short Term Memory)는 cell state를 도입하여 그래디언트 문제를 해결하고자 함
  + 직전 시점 정보와 현 시점 정보를 더해줌으로써 그래디언트가 효과적으로 흐를 수 있게 한다.

+ Vanilla Seq2Seq는 소스 문장을 벡터화하는 encoder와 인코딩된 벡터를 타겟 문장으로 변환하는 decoder로 구성됨
+ 각 encoder와 decoder는 LSTM 셀을 사용함
+ 문장 길이가 길고 층이 깊으면, encoder가 압축해야할 정보가 너무 많아져 정보 손실이 일어나고, decoder는 encoder가 압축한 정보를 초반 예측에만 사용하는 경향을 보임
+ 이 때문에 bottle-neck 문제가 발생한다고 칭하며 attention mechanism이 도입됨

+ 앞에서 뒤, 뒤에서 앞을 모두 고려하는 양방향의 네트워크를 사용하면 성능 개선에 도움이 될 수 있음

### LSTM 셀을 사용하고, encoder에 양방향 네트워크, decoder에 attention mechanism을 적용한 BiLSTM with attention 모델
+ 기계번역에서 특히 좋은 성능을 냄
  + end-to-end 학습 : output에 대한 손실을 최소화하는 과정에서 모든 파라미터들이 동시에 학습됨
  + 분산 표상 (Distributed representation) : 단어와 구(phrase) 간 유사성을 입력 벡터에 내재화해 성능을 개선함
  + 개선된 문맥 탐색 (exploitation) : LSTM과 attention으로 문장의 길이가 길어져도 성능 저하를 막을 수 있음
  + 다범주 분류에 좋은 성능을 내는 딥러닝 기법을 사용하여 문장 생성 능력이 개선됨

### BiLSTM을 적용한 연구 
+ Chen et al.(2016) : 문맥에서 질의에 대한 응답을 찾는 모델 구축
+ Eric&Manning (2017) : attention score가 높은 encoder 입력 단어를, decoder 입력에 복사해 넣으면서 decoder의 입력을 학습시키는 기법 개발


# 딥러닝 기반 자연어처리 기법의 최근 연구동향
Young, T., Hazarika, D., Poria, S., & Cambria, E. (2017). Recent Trends in Deep Learning Based Natural Language Processing. arXiv preprint arXiv:1708.02709. 논문 내용

### Abstract
+ 본 논문에서는 최근 수많은 NLP 과제에 적용되고 있는 딥러닝 모델과 기법을 검토하고, 성능에 대해 논함
+ 다양한 모델을 비교하고 대조함으로써 딥러닝 기반의 NLP 기법에 대해 이해하고자 함

### 서론
+ NLP : 인간 언어 분석과 표현을 자동화하기 위한 계산 기법

### 분산 표상
+ 통계 기반의 자연어처리 기법은 복잡한 자연어를 모델링하는 데 기본 옵션으로 부상했으나, 초기에는 차원의 저주로 어려움을 겪었음
+ 차원의 저주는 저차원 벡터 공간에 존재하는 단어의 분산 표상을 학습하는 연구의 동기가 됨

#### A. 단어 임베딩
+ 분산표상으로 표현된 벡터 또는 단어 임베딩은 비슷한 의미를 지닌 단어는 비슷한 문맥에 등장하는 경향이 있을 것임
  + 이에 의해 이웃한 단어의 특징을 잡아내고자, 코사인 유사도 등의 지표를 사용하며 벡터간의 유사성을 측정함
+ 딥러닝 모델의 첫번째 데이터 처리 계층에 자주 사용됨

#### B. Word2Vec
+ CBOW : k개 만큼의 주변 단어가 주어졌을 때, 중심 단어의 조건부 확률을 계산함
+ skip-gram : 중심 단어가 주어졌을 때, 주변 단어를 예측함

#### C. 문자 임베딩
+ 단어 임베딩은 문법적, 의미적 정보를 잡아내지만, 품사 태깅이나 개체명 인식 등의 테스크에서는 단어 내부의 형태 정보 또한 중요함
+ 이런 배경에서 문자 수준의 임베딩이 적용됨
+ 미등재 단어 이슈에 자연스럽게 대처할 수 있음

### 컨볼루션 신경망
+ 워드 임베딩 이후, 단어 결합이나 n-gram으로부터 높은 수준의 feature를 추출해내기 위한 효율적인 함수의 필요성이 증대되며 CNN을 적용하게 됨

#### A. CNN 기본구조
+ 문장 모델링
+ 윈도우 접근법 : 완전한 자연어 문장을 벡터로 표현하는 cnn 아키텍체 이외에, 개체명 인식, 품사 태깅, SRL 등의 많은 NLP 문제는 단어 단위의 예측이 필요함. 이런 task에 적용하는 방식
  + 단어의 범주(tag)가 기본적으로 이웃 단어에 의존할 것이라고 가정하여 각 단어에 대해, 고정된 크기의 윈도우가 가정되고, 윈도우 내에 있는 하위 문장들이 고려됨

#### B. CNN 어플리케이션
NLP에 CNN을 적용한 주요 연구 소개

### RNN
+ 순차적인 정보를 처리하는 네트워크
+ 모든 입력값을 독립으로 가정

#### A. RNN 필요성
+ 언어에서 고유한 순차적인 성격을 포착할 수 있음
+ 매우 긴 문장, 단락, 심지어 문서까지 다양한 텍스트 길이를 모델링할 수 있음
+ 시간 분산 조인트 처리(time distributed joint processing)을 위한 네트워크 지원

#### B. 네트워크 구조
+ 기본 구조 : NLP 맥락에서의 RNN은 주로 Elman network에 기반을 두어 3개의 계층으로 이루어져 있음
+ LSTM : 간단한 RNN에 forget gate을 추가하여 배니싱 그래디언트, 익스플로딩 그래디언트 문제를 극복함
+ Gated Recurrent Units(GRU) : RNN의 변형으로, reset gate과 update gate으로 구성되어 lstm처럼 메모리를 

#### C. 어플리케이션
+ 단어 수준 분류
+ 문장 수준 분류
+ 문장 생성

#### D. 어텐션 메커니즘
+ encoder가 해당 작업과 완전히 관련되지 않은 정보까지 인코딩해야하는 경우가 있음
+ 입력값이 길거나 정보가 많은 경우, 선택적 인코딩이 불가능한 경우도 존재
+ 이런 경우를 개선하기 위해 decoder가 입력 시퀀스를 다시 참조할 수 있도록 함

### Recursive Neural Networks
+ 시퀀스 모델링에 강점을 지닌 기법
+ 문장의 문법적 구조 해석을 위해 트리 구조의 모델 사용

### 강화학습과 비지도학습
#### A. 문장 생성을 위한 강화학습
+ 강화학습 : 보상을 얻기 전에 행동을 수행하도록 에이전트를 학습시키는 기법
+ NLP 시스템을 학습하는 자연스러운 방법 제시

#### B. 비지도학습 기반 문장 표현
+ 데이터에서 풍부한 언어 구조를 학습할 수 있도록 함

#### C. 심층 생성 모델
+ VAE, GAN

### 메모리 네트워크
+ 어텐션(attention) 매커니즘은 인코더에서 만들어진 히든 벡터(hidden vector)들을 저장함
+ 디코더는 각 토큰을 생성하고 있는 중에 이 벡터들에 액세스할 수 있음
+ 이때 인코더의 히든 벡터들은 모델의 ‘내부 메모리(internal memory)’ 항목으로 볼 수 있음
+ 모델이 상호작용할 수 있는 메모리의 형태로 뉴럴 네트워크들을 연결하는 것에 대한 관심이 급증함

+ Weston et al.(2014) : 질의응답(QA)를 위한 메모리 네트워크 제안
+ Sukhbaatar et al.(2015) : 언어 모델링을 위한 모델의 특별한 사용법 제안
  + 문장의 각 단어가 메모리 항모으로 간주됨
  + 멀티홉(multiple hop)을 사용
+ Dynamic Memort Networks(2015) : 입력 표현, 어텐션 및 응답 매커니즘을 위한 뉴럴 넷 사용으로 이전의 메모리 기반 모델 개선 

### 기법별 성능
+ 생략

### 향후 트렌드
+ 강화학습과 비지도 학습을 활용한 NLP 어플리케이션이 더 많이 나올 것으로 예상
+ 내부 메모리(데이터에서 배운 상향적 지식)가 외부 메모리(지식베이스로부터 상속된 지식)로 풍부해진 보다 깊은 학습모델이 나오기를 기대
 

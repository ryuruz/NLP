# 한국어 임베딩
## 임베딩(embedding)

+ 자연어를 숫자의 나열인 벡터로 바꾼 결과 혹은 그 일련의 과정 전체를 가리키는 용어
+ 말뭉치(corpus)의 의미, 문법 정보가 응축되어 있음
+ 벡터 -> 사칙연산 가능, 단어/문서 관련도(relevance) 계산 가능
+ 임베딩을 통해 __전이학습__ 가능
  + 전이 학습(Transfer Learning) : 특정 문제를 풀기 위해 학습한 모델을 다른 문제를 푸는 데에 재사용하는 기법
  + 대규모 말뭉치를 pretrain한 임베딩을 문서 분류 모델의 입력값으로 쓰고, 해당 임베딩을 포함한 모델 전체를 문서 분류 과제를 잘 할 수 있도록 fine tunning하는 방식

# XLNet
+ 2019년 구글 연구팀이 발표한 기법

+ 이 연구팀에서 임베딩의 최근 흐름을 크게 두 가지로 구분
  + AR(AutoRegressive) model : 데이터를 순차적으로 처리하는 기법의 총칭
    + ELMo, GPT 등 : 이전 문맥을 바탕으로 다음 단어를 예측하는 과정에서 학습하는 모델
  + 문맥을 양방향(bidirectional)으로 볼 수 없는 한계가 있음
  + AE(AutoEncoding) model : 입력값을 복원하는 기법을 두루 일컬음
    + BERT : 문장 일부에 노이즈(마스킹)를 주어 문장을 원래대로 복원하는 과정에서 학습하는 모델
      + 마스킹 처리된 단어가 어떤 단어일지 맞추는 것에 포커스를 둔다는 점에서 DAE라고 표현하기도 함
    + 양방향 모델이기는 하나, 마스킹 처리한 토큰들을 서로 독립으로 가정하기 때문에 각 토큰들의 의존 관계를 따질 수 없다는 한계가 있음
    
+ 두 모델의 한계를 극복하기 위해 Permutation Language model을 제안
  + 토큰을 랜덤으로 셔플한 뒤 그 뒤바뀐 순서를 기반으로 언어 모델을 학습하는 기법
  + 특정 토큰을 예측할 때 문장 전체 문맥을 살필 수 있게 됨
    + 해당 토큰을 제외한 문장의 부분집합 전부를 학습할 수 있다는 것

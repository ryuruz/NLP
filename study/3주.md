# 그래프로 중요 기사 걸러내기

## Structured Journalism 
+ 기사 및 기사 속에 내재된 정보를 계속 누적시켜 재맥락화(recontextualize)하려는 노력을 총칭함

## Graph-based News Representation
+ 아래 전제를 만족하는 모델
    + 중요한 뉴스라면 모든 언론사가 취급할 것이다
    + 중요 기사는 제목이나 키워드가 비슷할 것이다
+ 뉴스 제목을 노드, 엣지로 표현
+ 중복 단어 숫자를 웨이트로 하는 무방향 엣지
+ 노드의 중요도를 뽑는 개념 - 중심성(Centrality)
    + 간선 중심성(Degree Centrality) : 각 노드의 중심성 스코어는 엣지에 해당하는 웨이트들의 합
    + 고유벡터 중심성(Eigenvector Centrality) : 그래프의 엣지로 이뤄진 인접 행렬을 고유값 분해를 통해 얻을 수 있음
        + 중요한 노드에 연결된 노드가 중요하는 관점에서 창안
        + 중심성 점수가 높은 중요한 이웃과 연결된 i번째 노드 또한 중심성 점수가 높아지게 됨
+ 간선 중심성과 고유벡터 중심성 두 가지 지표를 곱하여, 모두 고려

___
# Sequence-to-Sequence 모델로 뉴스 제목 추출하기

## Sequence-to-Sequence (S2S) 
+ Recurrent Neural Network 중 하나로, LSTM GRU 등 RNN cell을 깊고 길게 쌓아 복잡하고 방대한 시퀀스 데이터를 처리하는 데 특화된 모델
+ 영어-한국어, 한국어-일본어 등 기계 번역에 쓰이고 있음
+ 정답이 있는 데이터만 학습이 가능

## 구성
+ 인코더(Encoder) : Source Language인 영어 텍스트를 처리
    + 정보를 압축
+ 디코더(Decoder) : Target Language인 한국어 텍스트를 처리
    + 인코더가 보내온 정보와 실제 정답을 입력으로 받아, output 출력
+ __인코더에 뉴스 본문을 넣고, 디코더에 제목을 넣음__

## 데이터 전처리
1. 기사를 제목과 본문으로 나눔
2. 단어를 띄어쓰기 귀준으로 나눈 후, 3글자 까지만 잘라 normalize 진행
+ 단어 수를 줄여 분석의 효율성을 높이기 위함
+ ex) [감정가, 감정가의, 감정가격에, 감정가격은, 감정가격이] -> 모두 '감정가'로 한 단어로 취급
3. S2S에 넣을 input data 생성
+ 텍스트를 숫자로 바꾸어 넣어야 함
+ 따라서 텍스트와  숫자가 매칭된 사전 생성
+ ex) word_to_idx = {경기도 : 0 , 준예산 : 1,,,, 감정가 : 979, 감정가의 : 979, 감정가격에 : 979,,,,}
+ ex) idx_to_word = {0 : 가, 1 : 가가갤,,,, 979 : 감정가,,, }
+ 학습 말뭉치를 바로 3글자로 줄이지 않고, word_to_idx에 모든 단어를 넣은 뒤 같은 인덱스를 주어 idx_to_word를 통해 단어가 숫자로 변환될 때, 자연스럽게 3글자로 normalize될 수 있도록 함

## 모델 구현 (생략)
## 느낀점
+ S2S는 예측시 비교적 일반적인 단어를 출력하는 경우가 많다는 점이 흥미로움
+ S2S로 챗봇을 이용하는 경우 네, 그럼요 등의 일반적인(오답이 되기 어려운) 답들이 나온다는 것도 흥미로움

___
# RNN과 LSTM을 이해해보자!

## RNN (Recurrent Neural Networks)
+ 히든 노드가 방향을 가진 엣지로 연결되어 순환 구조를 이루는 인공신경망
+ 음성, 문자 등 순차적으로 등장하는 데이터 처리에 적합한 모델
+ 시퀀스 길이에 상관 없이 인풋과 아웃풋을 받아들일 수 있는 네트워크 구조
    + 따라서 필요에 따라 다양하고 유연하게 구조를 만들 수 있다는 장점이 있음
+ hidden layer의 활성 함수로 비선형 함수인 하이퍼볼릭탄젠트(tanh) 사용
    + _활성 함수로 선형 함수 사용시, 히든 레이어가 없는 네트워크로 표현할 수 있음 -> 층을 쌓는 혜택 x 

## LSTM
+ RNN의 __Vanishing Gradient Problem__을 극복하기 위해서 고안된 모델
    + 관련 정보와 그 정보를 사용하는 지점 사이의 거리가 멀 경우 역전파 시에 gradient가 감소하여 학습능력이 크게 저하는 문제 
+ RNN의 hidden state에 cell-state를 추가한 구조
    + cell- state : 일종의 컨베이어 벨트 역할, state가 꽤 오래 경과하더라도 gradient가 비교적 잘 전파됨

___
# Word2Vec으로 문장 분류하기
휴대폰 리뷰를 바탕으로, [배터리, 카메라, 디자인 •••] 등의 범주로 분류하는 문제

## Word2Vec
+ 단어를 벡터로 바꿔주는 임베딩 수행
+ 단어를 벡터화할 떄, 단어의 문맥적 의미를 보존함
+ 유클리디안 거리, 코사인 유사도 등을 이용하여 벡터로 바뀐 단어들 간의 거리를 측정하고, 거리가 가까울 경우 의미가 비슷한 단어임을 알 수 있음
